\section{Introduction}

Unsupervised learning is a type of technique that learns patterns from unlabeled data. Among the many problems addressed by this technique, clustering has been widely studied for decades, and still is today one of the most important challenges within unsupervised learning.
Included in the clustering problems, text clustering that aims to group semantically similar texts without using any type of supervision or manually assigned labels, has already proven to be beneficial in various applications such as news recommendations \cite{cluster1}, language models \cite{cluster2}, view \cite{cluster3}, and corpus summarization \cite{bo1,bo2}.
Clustering methods well known in the literature, such as K-means and Gaussian Mixture Models (GMM) depend on the distance measured in thefeature space, which tends to be ineffective for high-dimensional data that is commonly found in textual datasets. On the other hand, deep neural networks are gaining attention as an effective way to map data to a small representation space. However, many deep neural network techniques only learn the representation of features, and thus it is still necessary to use an external method (k-means, for example) for unsupervised labeling.
So, several transformer-based deep learning techniques, which today represent the state-of-the-art in several Natural Language Processing (NLP) applications \cite{roberta,bert,dBert}, have been receiving less importance in text clustering works . A factor that prevents the use of many of these techniques in text clustering problems is precisely the fact that many of these methods do not have a final form of labeling, therefore needing an external method to group the features represented by these methods.
Thereby, several recent works present solutions using specific deep learning architectures for text clustering, such as topic modeling\cite{nvdm,nvdm2,neuraltopic} and short text clustering\cite{selfclu,nvlda}. On the other hand, several of these architectures seek to solve their specific text clustering problems, and thus, today in the literature there is a lack of comparison between specific methods\cite{selfclu,neuraltopic,topicS,topicNeu} and methods that can deal with text clustering in general\cite{xlm_roberta,dBert}. 

In this paper, we propose a novel framework based on MVAE (mixture variational autoencoder) for general text clustering. Our model is an unsupervised generative model of text which aims to extract a continuous semantic latent variable for each document. Therefore, each mixture component models the joint distribution of the latent variables and the bag-of-words vector that represents a document. This distribution is represented as the graphical model known as the Boltzmann Machine, popular in NLP for performing well in a number of tasks and for being efficiently learned with variational learning due to its simple structure~\citep{bo3, bo2}. 

Our model, dubbed \model, can learn text representations from unlabeled data without requiring any pre-trained data.\model~takes as input the bag-of-word vector representing a document and outputs its latent representation. Our technique can be interpreted as a variation of a variational auto-encoder model. First, we use two MLP encoders (inference network) to compress documents and for mixture representation(cluster assign), and then we use a softmax decoder (generative model) to reconstruct the document by generating the words independently.

In this way, \model~can learn latent representations of texts and still enable labeling them. And so, in our experiments we will demonstrate how \model~can be used for clustering texts. And with that, we will demonstrate how the \model~ managed to overcome, presenting a less complex and simple approach to training, several works that today represent the state of the art in text clustering. 

Therefore, in our experiments using more than 8 textual datasets we will examine how our method presented superior results in different scenarios using several unsupervised metrics.Finally, we can list some of the findings of this work:
\begin{itemize}
    \item  We propose \model, a new unsupervised model for document representation based on mixture variational autoencoders, which can also be used for clustering
    \item We compare our technique with various state-of-the-art baselines in NLP. We commit to make all the source code available for the purpose of use and reproducibility\footnote{upon acceptance}.
\end{itemize}
